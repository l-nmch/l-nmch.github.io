---
date: '2025-06-27T22:03:04+02:00'
title: 'Mon premier projet avec un NPU (Hailoâ€‘8L) : de MNIST Ã  lâ€™infÃ©rence embarquÃ©e'
description: "Un enfer pour comprendre ce qu'est un NPU, mon setup matÃ©riel et logiciel, et comment j'ai fait tourner mon propre modÃ¨le MNIST sur un Hailoâ€‘8L."
tags: ["NPU", "Hailo-8L", "MNIST", "AI", "Raspberry Pi", "Inference"]
---

# Mon premier projet avec un NPU (Hailoâ€‘8L) : de MNIST Ã  lâ€™infÃ©rence embarquÃ©e

---

## ğŸ¯ Pourquoi ce projet ?

Au dÃ©part, je pensais que le Hailoâ€‘8L câ€™Ã©tait un genre de puce magique : OÃ¹ tu peux run des LLM, l'utilisÃ© en tant que device sur des framework comme [Tensorflow](https://tensorflow.org), faire de l'entrainement de model,

Spoiler : Absolument pas

En creusant un peu, jâ€™ai compris que le Hailoâ€‘8L est fait *uniquement* pour faire de lâ€™**infÃ©rence**, pas dâ€™entraÃ®nement, et encore moins du [Reinforcement Learning] ou autre LLM en tout genre.

Câ€™est une puce ultra-efficace, mais spÃ©cialisÃ©e, conÃ§ue pour faire tourner des modÃ¨les dÃ©jÃ  entraÃ®nÃ©s sur des systÃ¨mes embarquÃ©s (camÃ©ras, robots, edge devices...).

Alors jâ€™ai testÃ© les modÃ¨les prÃ©-existants de Hailo â€” et câ€™Ã©tait quasiment que des modÃ¨les de vision par ordinateur : classification dâ€™images, dÃ©tection dâ€™objets, segmentation, etc.

Sauf qu'honnetement, moi jâ€™me fous un peu de Ã§a. Ce que je voulais, câ€™Ã©tait **coder mon propre modÃ¨le IA**, comprendre ce quâ€™il y a sous le capot, et le faire tourner sur mon hardware.

Du coup, jâ€™ai dÃ©cidÃ© de partir from scratch, avec **MNIST**, le dataset â€œHello Worldâ€ du deep learning. Mon objectif :

- CrÃ©er mon tout premier modÃ¨le de A Ã  Z  
- Le faire tourner en local sur le Hailoâ€‘8L  
- Comprendre toute la chaÃ®ne de bout en bout

Si tu veux jeter un Å“il au code complet, tout est dispo ğŸ‘‰ [ici](https://github.com/l-nmch/hailo-mnist).

---

## ğŸ”§ Mon setup (hardware + software)

### ğŸ§± CÃ´tÃ© matos

Jâ€™utilise un **Raspberry Pi 5**, tout simplement parce que câ€™est le *seul* Pi qui est compatible avec le NPU **Hailoâ€‘8L**.  
Pourquoi ? Parce que le Hailoâ€‘8L se connecte en **PCIe**, et câ€™est le seul Pi qui expose une vraie ligne PCIe accessible facilement.

Le NPU est branchÃ© via une carte nape **PCIe x1**. Tout est expliquÃ© dans la doc [Raspberry Pi](https://www.raspberrypi.com/documentation/accessories/ai-hat-plus.html#ai-hat-plus), que jâ€™ai suivie pour lâ€™install des paquets cÃ´tÃ© Raspberry.

![Setup matÃ©riel : Raspberry Pi 5 + Hailo-8L](https://www.raspberrypi.com/documentation/accessories/images/ai-hat-plus-hero.jpg?)  

---

### ğŸ’» CÃ´tÃ© software

Sur le Pi :  

- Installation des **paquets Hailo (SDK + runtime)** pour pouvoir faire tourner les modÃ¨les  
- Utilisation de **HailoRT** pour charger le modÃ¨le sur le NPU et lancer lâ€™infÃ©rence

Mais pour la **compilation du modÃ¨le**, câ€™est un peu plus chaud :  
Le **Hailo Dataflow Compiler (DFC)**, qui permet de convertir un modÃ¨le TensorFlow ou ONNX vers un exÃ©cutable `.hef` pour le NPU, **ne tourne que sur x86_64** et est disponible en source fermÃ© sur [Hailo Developper Zone](https://hailo.ai/developer-zone/).

Du coup jâ€™ai tout mis en place sur mon PC perso (sous Pop OS), et câ€™est lÃ  que jâ€™ai entraÃ®nÃ©, exportÃ© et quantifiÃ© mon modÃ¨le.

---

### ğŸ¤” Petite dÃ©couverte au passage

Au dÃ©but je comprenais rien Ã  leurs specs : ils parlaient de **"26 TOPS"**, mais pas de TFLOPS, et je comprenais pas trop le dÃ©lire.  
En fait câ€™est parce que le Hailoâ€‘8L **ne fait pas de float du tout**. Il bosse exclusivement en **INT8** (entiers 8 bits), ce qui explique pourquoi il est aussi rapide *et* aussi Ã©conome.

Donc si tu veux que ton modÃ¨le tourne dessus, faut **quantifier** ton modÃ¨le float16 â†’ int8 via le DFC, sinon câ€™est mort.

---

## âš™ï¸ Les grandes Ã©tapes du projet

### 1ï¸âƒ£ EntraÃ®nement du modÃ¨le MNIST

Jâ€™ai commencÃ© par crÃ©er un modÃ¨le simple, stable et fonctionnel, en lâ€™entraÃ®nant sur le dataset classique **MNIST** (les chiffres manuscrits).  
Pour Ã§a, jâ€™ai utilisÃ© **Keras**, parce que câ€™est simple et efficace pour prototyper vite.

![SchÃ©ma simple du modÃ¨le CNN pour MNIST](/img/CNN.png)

---

### 2ï¸âƒ£ Exporter le modÃ¨le

Une fois entraÃ®nÃ©, il a fallu voir dans quel format je pouvais exporter ce modÃ¨le pour pouvoir le convertir ensuite avec les outils Hailo.  
Jâ€™ai explorÃ© plusieurs formats :

- `.keras` (le format natif Keras)  
- `.tflite` (TensorFlow Lite, souvent utilisÃ© pour lâ€™embarquÃ©)  
- **`.onnx`** (Open Neural Network Exchange), qui est **le plus commun et supportÃ© par la plupart des frameworks et outils**, donc câ€™est celui que jâ€™ai privilÃ©giÃ©.

---

### 3ï¸âƒ£ DÃ©couverte et prise en main du Hailo DFC

Ensuite, je me suis familiarisÃ© avec le **Dataflow Compiler (DFC)** de Hailo.  
Cet outil prend le fichier `.onnx` ou `.tflite` et le compile en un format propriÃ©taire `.hef` optimisÃ© pour le NPU Hailo.  
En plus, il permet de faire la **quantification en INT8** obligatoire pour ce hardware.

![SchÃ©ma du pipeline compilation avec DFC](/img/DFC_Diagram.png)

---

### 4ï¸âƒ£ Compilation et dÃ©ploiement sur le Raspberry Pi

Avec mon fichier `.hef` en poche, jâ€™ai transfÃ©rÃ© tout Ã§a sur mon Raspberry Pi 5 Ã©quipÃ© du NPU.  
GrÃ¢ce au **runtime HailoRT**, jâ€™ai pu lancer lâ€™infÃ©rence directement sur la puce, et valider que tout tournait bien.  
Ã‡a fait un peu geek, mais voir ton modÃ¨le tourner sur une puce embarquÃ©e, câ€™est un vrai kifâ€¯! ğŸ˜

---

## ğŸ’¡ Ce que jâ€™ai appris / Ã€ retenir

Un truc hyper important Ã  comprendre avec les NPU (que ce soit le Hailoâ€‘8L, le Google Edge TPU Coral, ou dâ€™autres), câ€™est quâ€™ils **ne font pas dâ€™entraÃ®nement**.  
Ce sont des puces **uniquement dÃ©diÃ©es Ã  lâ€™infÃ©rence** â€” câ€™est-Ã -dire quâ€™elles exÃ©cutent des modÃ¨les dÃ©jÃ  entraÃ®nÃ©s, en mode super optimisÃ© et basse consommation.

Du coup, il faut toujours :

- Dâ€™abord **entraÃ®ner ton modÃ¨le** sur un PC ou serveur classique quitte Ã  avoir des GPU
- Ensuite **compiler** ce modÃ¨le avec un outil spÃ©cialisÃ© (comme le Dataflow Compiler chez Hailo) pour gÃ©nÃ©rer un fichier exÃ©cutable compatible avec la puce  
- Puis **dÃ©ployer** ce fichier compilÃ© sur la puce pour faire lâ€™infÃ©rence en temps rÃ©el

Si tu tâ€™attends Ã  faire de lâ€™entraÃ®nement directement sur un NPU, tu vas perdre du temps. Ces puces sont faites pour dÃ©ployer du ML Ã  la vitesse de lâ€™Ã©clair, pas pour apprendre de nouvelles donnÃ©es.

---

## ğŸ“ Liens utiles

- Le repo complet (code + tuto dÃ©taillÃ©) : [github.com/l-nmch/hailo-mnist](https://github.com/l-nmch/hailo-mnist)  
- SDK Hailo : [hailo.ai](https://hailo.ai)  
- Forum Hailo (trÃ¨s utile !) : [community.hailo.ai](https://community.hailo.ai)


## ğŸ§  Terminologie

| Terme                  | DÃ©finition rapide |
|------------------------|-------------------|
| **NPU** (Neural Processing Unit) | Processeur spÃ©cialisÃ© pour exÃ©cuter rapidement des modÃ¨les dâ€™IA, en particulier des rÃ©seaux de neurones. OptimisÃ© pour lâ€™infÃ©rence. |
| **InfÃ©rence**          | Ã‰tape oÃ¹ le modÃ¨le, dÃ©jÃ  entraÃ®nÃ©, est utilisÃ© pour faire des prÃ©dictions (par ex. reconnaÃ®tre un chiffre). |
| **EntraÃ®nement**       | Phase oÃ¹ le modÃ¨le apprend Ã  partir de donnÃ©es. NÃ©cessite gÃ©nÃ©ralement beaucoup de ressources (GPU/TPU). |
| **INT8**               | Format numÃ©rique entier codÃ© sur 8 bits. Moins prÃ©cis que float32, mais plus rapide et lÃ©ger pour les NPU. |
| **Quantification**     | Processus de conversion dâ€™un modÃ¨le entraÃ®nÃ© en float32/float16 vers INT8 pour quâ€™il soit exÃ©cutable sur du hardware spÃ©cialisÃ©. |
| **TOPS** (Tera-Operations Per Second) | UnitÃ© de mesure des performances pour les puces IA. DiffÃ©rent de TFLOPS car souvent utilisÃ© avec des opÃ©rations INT8. |
| **ONNX** (Open Neural Network Exchange) | Format de modÃ¨le IA open-source, compatible avec plusieurs frameworks (TensorFlow, PyTorch, etc). |
| **.hef**               | Format compilÃ© propriÃ©taire utilisÃ© par Hailo pour ses NPU. GÃ©nÃ©rÃ© par le Dataflow Compiler. |
| **HailoRT**            | Runtime fourni par Hailo pour charger un `.hef` sur le NPU et exÃ©cuter les infÃ©rences. |
| **Dataflow Compiler (DFC)** | Outil de Hailo (PC x86_64 only) pour compiler et quantifier les modÃ¨les vers le format `.hef`. |

---

## âœ… Conclusion

Jâ€™espÃ¨re que cette mini-aventure tâ€™a donnÃ© envie de tester du edge AI toi aussi.  
Si tâ€™as un Hailoâ€‘8L ou autre NPU sous la main, franchement, fonce.  
Et si tu veux reprendre mon code pour tester, go : câ€™est open-source ğŸ’¡